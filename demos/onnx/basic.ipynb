{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX 模型基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import sys\n",
    "import onnx\n",
    "from onnx.mapping import TENSOR_TYPE_TO_NP_TYPE\n",
    "from collections import defaultdict\n",
    "\n",
    "def inspect_onnx(model_path): \n",
    "    onnx_model = onnx.load(model_path)\n",
    "    print(f\"====== load model {model_path} done.\")\n",
    "\n",
    "    g = onnx_model.graph\n",
    "\n",
    "    def process_node():\n",
    "        op_counter = defaultdict(int)\n",
    "        for node in g.node:\n",
    "            name = node.name\n",
    "            optype = name.split(\"/\")[-1].split(\"_\")[0]\n",
    "            optype = node.op_type\n",
    "            op_counter[optype] += 1\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        for k, v in sorted(op_counter.items()):\n",
    "            print(f\"{k:40}:  {v}\")\n",
    "        print(\"*\" * 50)\n",
    "        print(\"Total OP num: \", reduce(lambda i, j: i + j, op_counter.values(), 0))\n",
    "\n",
    "    def process_inout():\n",
    "        print(\"/n\", \"*\" * 50, \"inputs:\")\n",
    "        for node in g.input:\n",
    "            name = node.name\n",
    "            dtype = str(TENSOR_TYPE_TO_NP_TYPE[node.type.tensor_type.elem_type])\n",
    "            shape = [dim.dim_value for dim in node.type.tensor_type.shape.dim]\n",
    "            print(f\"{name:40}{dtype:<10}{shape}\")\n",
    "            \n",
    "        print(\"/n\", \"*\" * 50, \"outputs:\")\n",
    "        for node in g.output:\n",
    "            name = node.name\n",
    "            dtype = str(TENSOR_TYPE_TO_NP_TYPE[node.type.tensor_type.elem_type])\n",
    "            shape = [dim.dim_value for dim in node.type.tensor_type.shape.dim]\n",
    "            print(f\"{name:40}{dtype:<10}{shape}\")\n",
    "\n",
    "    def nodename_info():\n",
    "\n",
    "        node_name_set = set(n.name for n in g.node) \n",
    "        valinfo_name_set = set(n.name for n in g.value_info)\n",
    "\n",
    "\n",
    "        print(f\"node  name len: {len(node_name_set)}\") \n",
    "        print(f\"vainf name len: {len(valinfo_name_set)}\")\n",
    "\n",
    "        a = node_name_set.update(valinfo_name_set)\n",
    "        print(f\"merge name len: {len(node_name_set)}\")\n",
    "    \n",
    "    process_node()\n",
    "    process_inout()\n",
    "    nodename_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import datetime\n",
    "def infer_onnx(model_path):\n",
    "    # Load the ONNX model\n",
    "    sess = ort.InferenceSession(model_path)\n",
    "    \n",
    "    feeds = {}\n",
    "    for input in sess.get_inputs():\n",
    "        # Get input name and shape\n",
    "        input_name = input.name\n",
    "        input_shape = input.shape\n",
    "        numpy_type = TENSOR_TYPE_TO_NP_TYPE[input.type]\n",
    "        # Generate random input\n",
    "        input_data = np.random.random_sample(input_shape).astype(numpy_type)\n",
    "\n",
    "        feeds[input_name] = input_data\n",
    "\n",
    "    # Run the model\n",
    "    start = datetime.datetime.now()\n",
    "    result = sess.run(None, feeds)\n",
    "    print(datetime.datetime.now() - start)\n",
    "\n",
    "    print([a.shape for a in result])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX 模型构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "from onnx import helper \n",
    "from onnx import TensorProto \n",
    " \n",
    "# input and output \n",
    "a = helper.make_tensor_value_info('a', TensorProto.FLOAT, [10, 10]) \n",
    "x = helper.make_tensor_value_info('x', TensorProto.FLOAT, [10, 10]) \n",
    "b = helper.make_tensor_value_info('b', TensorProto.FLOAT, [10, 10]) \n",
    "output = helper.make_tensor_value_info('output', TensorProto.FLOAT, [10, 10]) \n",
    " \n",
    "# Mul \n",
    "mul = helper.make_node('Mul', ['a', 'x'], ['c']) \n",
    " \n",
    "# Add \n",
    "add = helper.make_node('Add', ['c', 'b'], ['output']) \n",
    " \n",
    "# graph and model \n",
    "graph = helper.make_graph([mul, add], 'linear_func', [a, x, b], [output]) \n",
    "model = helper.make_model(graph) \n",
    " \n",
    "# save model \n",
    "onnx.checker.check_model(model) \n",
    "print(model) \n",
    "onnx.save(model, 'linear_func.onnx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime \n",
    "import numpy as np \n",
    " \n",
    "sess = onnxruntime.InferenceSession('linear_func.onnx') \n",
    "a = np.random.rand(10, 10).astype(np.float32) \n",
    "b = np.random.rand(10, 10).astype(np.float32) \n",
    "x = np.random.rand(10, 10).astype(np.float32) \n",
    " \n",
    "output = sess.run(['output'], {'a': a, 'b': b, 'x': x})[0] \n",
    " \n",
    "assert np.allclose(output, a * x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX 模型修改\n",
    "\n",
    "**NOTE**: 下面的例子将Add改成Sub，一般如果不是类似的OP，直接修改op_type可能会有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "model = onnx.load('linear_func.onnx') \n",
    " \n",
    "node = model.graph.node \n",
    "node[1].op_type = 'Sub' \n",
    " \n",
    "onnx.checker.check_model(model) \n",
    "print(model)\n",
    "onnx.save(model, 'linear_func_2.onnx')\n",
    "\n",
    "import onnxruntime \n",
    "import numpy as np \n",
    " \n",
    "sess = onnxruntime.InferenceSession('linear_func_2.onnx') \n",
    "a = np.random.rand(10, 10).astype(np.float32) \n",
    "b = np.random.rand(10, 10).astype(np.float32) \n",
    "x = np.random.rand(10, 10).astype(np.float32) \n",
    " \n",
    "output = sess.run(['output'], {'a': a, 'b': b, 'x': x})[0] \n",
    " \n",
    "assert np.allclose(output, a * x - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX 子图提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx  \n",
    " \n",
    "onnx.utils.extract_model('linear_func_2.onnx', 'mul.onnx', ['a','x'], ['c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "from pprint import pprint\n",
    "from onnx.mapping import TENSOR_TYPE_TO_NP_TYPE,NP_TYPE_TO_TENSOR_TYPE\n",
    "\n",
    "pprint(TENSOR_TYPE_TO_NP_TYPE)\n",
    "pprint(NP_TYPE_TO_TENSOR_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Date         </span>┃<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Title                             </span>┃<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Production Budget </span>┃<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">     Box Office </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Dev 20, 2019 </span>│ Star Wars: The Rise of Skywalker  │      $275,000,000 │   $375,126,118 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> May 25, 2018 </span>│ <span style=\"color: #800000; text-decoration-color: #800000\">Solo</span>: A Star Wars Story           │      $275,000,000 │   $393,151,347 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Dec 15, 2017 </span>│ Star Wars Ep. VIII: The Last Jedi │      $262,000,000 │ <span style=\"font-weight: bold\">$1,332,539,889</span> │\n",
       "└──────────────┴───────────────────────────────────┴───────────────────┴────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;32m \u001b[0m\u001b[1;32mDate        \u001b[0m\u001b[1;32m \u001b[0m┃\u001b[1;32m \u001b[0m\u001b[1;32mTitle                            \u001b[0m\u001b[1;32m \u001b[0m┃\u001b[1;32m \u001b[0m\u001b[1;32mProduction Budget\u001b[0m\u001b[1;32m \u001b[0m┃\u001b[1;32m \u001b[0m\u001b[1;32m    Box Office\u001b[0m\u001b[1;32m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2mDev 20, 2019\u001b[0m\u001b[2m \u001b[0m│ Star Wars: The Rise of Skywalker  │      $275,000,000 │   $375,126,118 │\n",
       "│\u001b[2m \u001b[0m\u001b[2mMay 25, 2018\u001b[0m\u001b[2m \u001b[0m│ \u001b[31mSolo\u001b[0m: A Star Wars Story           │      $275,000,000 │   $393,151,347 │\n",
       "│\u001b[2m \u001b[0m\u001b[2mDec 15, 2017\u001b[0m\u001b[2m \u001b[0m│ Star Wars Ep. VIII: The Last Jedi │      $262,000,000 │ \u001b[1m$1,332,539,889\u001b[0m │\n",
       "└──────────────┴───────────────────────────────────┴───────────────────┴────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.table import Column, Table\n",
    "\n",
    "console = Console()\n",
    "\n",
    "table = Table(show_header=True, header_style=\"bold green\")\n",
    "table.add_column(\"Date\", style=\"dim\", width=12)\n",
    "table.add_column(\"Title\")\n",
    "table.add_column(\"Production Budget\", justify=\"right\")\n",
    "table.add_column(\"Box Office\", justify=\"right\")\n",
    "table.add_row(\n",
    "    \"Dev 20, 2019\", \"Star Wars: The Rise of Skywalker\", \"$275,000,000\", \"$375,126,118\"\n",
    ")\n",
    "table.add_row(\n",
    "    \"May 25, 2018\",\n",
    "    \"[red]Solo[/red]: A Star Wars Story\",\n",
    "    \"$275,000,000\",\n",
    "    \"$393,151,347\",\n",
    ")\n",
    "table.add_row(\n",
    "    \"Dec 15, 2017\",\n",
    "    \"Star Wars Ep. VIII: The Last Jedi\",\n",
    "    \"$262,000,000\",\n",
    "    \"[bold]$1,332,539,889[/bold]\",\n",
    ")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "from onnx import shape_inference, TensorProto\n",
    "\n",
    "\n",
    "m = onnx.load(\"C:/Users/jianlong/win24_models/QDQ-Models/Model-G3-QDQ/Model-G3-QDQ-v1_1/Model_G3.quant.onnx\")\n",
    "inferred_model = shape_inference.infer_shapes(m)\n",
    "onnx.save(inferred_model, \"save.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidGraph",
     "evalue": "[ONNXRuntimeError] : 10 : INVALID_GRAPH : Load model from C:/Users/jianlong/win24_models/QDQ-Models/Model-G3-QDQ/Model-G3-QDQ-v1_1/Model_G3.quant.onnx failed:This is an invalid model. Type Error: Type 'tensor(uint16)' of input parameter (cache_frames_zero_point) of operator (QuantizeLinear) in node (cache_frames_QuantizeLinear) is invalid.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidGraph\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/jianlong/win24_models/QDQ-Models/Model-G3-QDQ/Model-G3-QDQ-v1_1/Model_G3.quant.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43minfer_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m, in \u001b[0;36minfer_onnx\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer_onnx\u001b[39m(model_path):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Load the ONNX model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     sess \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     feeds \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sess\u001b[38;5;241m.\u001b[39mget_inputs():\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m# Get input name and shape\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jianlong\\AppData\\Local\\miniconda3\\envs\\dl\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:383\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[1;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[1;32mc:\\Users\\jianlong\\AppData\\Local\\miniconda3\\envs\\dl\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:424\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[1;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[0;32m    422\u001b[0m session_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;28;01melse\u001b[39;00m C\u001b[38;5;241m.\u001b[39mget_default_session_options()\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n\u001b[1;32m--> 424\u001b[0m     sess \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_config_from_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    426\u001b[0m     sess \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mInferenceSession(session_options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_config_from_model)\n",
      "\u001b[1;31mInvalidGraph\u001b[0m: [ONNXRuntimeError] : 10 : INVALID_GRAPH : Load model from C:/Users/jianlong/win24_models/QDQ-Models/Model-G3-QDQ/Model-G3-QDQ-v1_1/Model_G3.quant.onnx failed:This is an invalid model. Type Error: Type 'tensor(uint16)' of input parameter (cache_frames_zero_point) of operator (QuantizeLinear) in node (cache_frames_QuantizeLinear) is invalid."
     ]
    }
   ],
   "source": [
    "a = \"C:/Users/jianlong/win24_models/QDQ-Models/Model-G3-QDQ/Model-G3-QDQ-v1_1/Model_G3.quant.onnx\"\n",
    "\n",
    "infer_onnx(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
